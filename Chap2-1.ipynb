{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过名为SparkSession的驱动器来控制Spark的应用程序，需要创建一个SparkSession实例来在集群中执行用户定义的操作，每一个Spark应用程序都需要一个SparkSession与之对应。\n",
    "\n",
    "也就是说，SparkSession就是在创建一个驱动器进程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-FEKP4SCG:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1ab7c1c4388>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一个Spark DataFrame，可以把它想成一个将数据存储在多台计算机上的电子表格\n",
    "\n",
    "当创建DataFrame的时候，大部分时候是不需要手动操作分区，只需要指定数据的高级转换操作，然后让Spark决定此工作如何在集群上运行即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|number|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myRange = spark.range(1000).toDF('number')\n",
    "myRange.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意！Spark的核心数据结构在计算过程中是保持不变的，这意味着他们在创建之后无法更改!!\n",
    "\n",
    "我们对于它的操作更像sql里面生成一个虚拟表，只是对他进行“转换”，在我们调用动作操作之前，Spark并不会真的执行转换操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisBy2 = myRange.where('number % 2 = 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 转换操作\n",
    "\n",
    "分类：\n",
    "窄依赖关系(narrow dependency),每个输入分区仅决定一个输出分区的转换，一个分区最多只会对一个输出分区有影响。\n",
    "\n",
    "宽依赖关系(wide dependency)，每个输入分区决定了多个输出分区，会在整个集群中执行互相交换分区数据的功能，称为洗牌(shuffle)，会将结果写入磁盘。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 惰性评估\n",
    "\n",
    "惰性评估(lazy evaluation)的意思是等到绝对需要时才执行计算。在Spark中，当用户表达一些对数据的操作时，不是立即修改数据，而是建立一个作用到原始数据的转换计划（只是记录下操作路径）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 动作操作\n",
    "动作操作是我们能够建立逻辑转换计划，为了触发计算，我们需要运行一个动作操作(action)，一个动作指示Spark在一系列转换操作后计算得到一个结果。\n",
    "\n",
    "有三类动作：\n",
    "\n",
    "在控制台中查看数据\n",
    "\n",
    "在某个语言中将数据汇集为原生对象\n",
    "\n",
    "写入输出数据源"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完整的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015 = spark.read.option('inferSchema', 'true').option('header', 'true').csv('./data/flight-data/csv/2015-summary.csv')\n",
    "flightData2015.show(5)\n",
    "flightData2015.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意这些并没有指定行数，因为读取数据也是一种转换操作，属于惰性操作。spark的Dataframe会根据数据来猜测每列应该是什么类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.sort('count')\n",
    "flightData2015.show(5)\n",
    "flightData2015.take(5)\n",
    "# 发现并没有改变，因为sort也属于转换，并没有实际对原数据进行改动。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然我们不知道发生了什么，但是可以利用explain函数来进行观察，阅读解释计划。解释计划从上往下看，上面是最终结果，下面是数据源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [count#20 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(count#20 ASC NULLS FIRST, 200)\n",
      "   +- *(1) FileScan csv [DEST_COUNTRY_NAME#18,ORIGIN_COUNTRY_NAME#19,count#20] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/D:/Jupyter Notebook Project/Pyspark/Spark-The-Definitive-Guide-master/dat..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "flightData2015.sort('count').explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过动作来触发计划的执行。默认情况下，shuffle会输出200哥shuffle分区，暂时将此值设置为5。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show 和 take 都一样，都属于在控制台上查看数据。要连续调用才可以。\n",
    "spark.conf.set('spark.sql.shuffle.patitions', '5')\n",
    "flightData2015.sort('count').show(5)\n",
    "flightData2015.sort('count').take(5)\n",
    "# 但是如果我们再倒回去发现并没有对原数据进行一个改变\n",
    "flightData2015.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sql操作\n",
    "想要使用Spark SQL， 可以将任何的DataFrame注册为数据表或者临时表，并使用纯SQL对它进行查询！！！这两者并没有性能差异！！！wow~\n",
    "\n",
    "可以看下面的输出发现exlpain执行的过程都是一样的，所以并没有差别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#18], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#18, 200)\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#18], functions=[partial_count(1)])\n",
      "      +- *(1) FileScan csv [DEST_COUNTRY_NAME#18] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/D:/Jupyter Notebook Project/Pyspark/Spark-The-Definitive-Guide-master/dat..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#18], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#18, 200)\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#18], functions=[partial_count(1)])\n",
      "      +- *(1) FileScan csv [DEST_COUNTRY_NAME#18] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/D:/Jupyter Notebook Project/Pyspark/Spark-The-Definitive-Guide-master/dat..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n"
     ]
    }
   ],
   "source": [
    "# 创建临时表\n",
    "flightData2015.createOrReplaceTempView('flight_data_2015')\n",
    "# sql查询\n",
    "sqlWay = spark.sql('''\n",
    "select DEST_COUNTRY_NAME, count(1)\n",
    "from flight_data_2015\n",
    "group by DEST_COUNTRY_NAME\n",
    "''')\n",
    "# dataframe查询\n",
    "dataframeWay = flightData2015.groupBy('DEST_COUNTRY_NAME').count()\n",
    "\n",
    "sqlWay.explain()\n",
    "dataframeWay.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用函数来进行简化操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|max(count)|\n",
      "+----------+\n",
      "|    370002|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "flightData2015.select(max('count')).show()\n",
    "flightData2015.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', destination_total=411352),\n",
       " Row(DEST_COUNTRY_NAME='Canada', destination_total=8399),\n",
       " Row(DEST_COUNTRY_NAME='Mexico', destination_total=7140),\n",
       " Row(DEST_COUNTRY_NAME='United Kingdom', destination_total=2025),\n",
       " Row(DEST_COUNTRY_NAME='Japan', destination_total=1548)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 找到count前五的信息\n",
    "# maxsql = spark.sql('''\n",
    "# select DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count\n",
    "# from flight_data_2015\n",
    "# order by count desc\n",
    "# limit 5\n",
    "# ''')\n",
    "# maxsql.take(5)\n",
    "\n",
    "\n",
    "maxSql = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "FROM flight_data_2015\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY sum(count) DESC\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "# 这里show会出错，所以改用take，不明白为什么show会出错，注意take必须传入选几行的参数，问题解决，是因为jdk版本的问题，之前版本太高，降了之后就行了！\n",
    "# maxSql.show()\n",
    "maxSql.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', destination_total=411352),\n",
       " Row(DEST_COUNTRY_NAME='Canada', destination_total=8399),\n",
       " Row(DEST_COUNTRY_NAME='Mexico', destination_total=7140),\n",
       " Row(DEST_COUNTRY_NAME='United Kingdom', destination_total=2025),\n",
       " Row(DEST_COUNTRY_NAME='Japan', destination_total=1548)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# 这个地方用show也会出问题，对比起来还是sql更好写\n",
    "flightData2015.groupBy(\"DEST_COUNTRY_NAME\").sum(\"count\").withColumnRenamed(\"sum(count)\", \"destination_total\").sort(desc(\"destination_total\")).limit(5).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=5, orderBy=[destination_total#467L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#18,destination_total#467L])\n",
      "+- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME#18], functions=[sum(cast(count#20 as bigint))])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#18, 200)\n",
      "      +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#18], functions=[partial_sum(cast(count#20 as bigint))])\n",
      "         +- *(1) FileScan csv [DEST_COUNTRY_NAME#18,count#20] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/D:/Jupyter Notebook Project/Pyspark/Spark-The-Definitive-Guide-master/dat..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "flightData2015.groupBy(\"DEST_COUNTRY_NAME\").sum(\"count\").withColumnRenamed(\"sum(count)\", \"destination_total\").sort(desc(\"destination_total\")).limit(5).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
